{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoGPT2_naver_movie(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNB4HzqNeLIrhGXiun7SRzQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2pterons/multicampus/blob/main/KoGPT2_naver_movie(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPKQHxbf36Kf",
        "outputId": "da6ca98b-d4c9-4462-c126-520bf5a76204"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"8-12.KoGPT2(gen_text).ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ABZelGJI7VxTiwsnmD6Fy3o85hxXfmw0\n",
        "\"\"\"\n",
        "\n",
        "# GPT2의 TFGPT2LMHeadModel 모델을 이용한 언어 생성\n",
        "# TFGPT2LMHeadModel : The GPT2 Model transformer with a language modeling head on top \n",
        "#                     (linear layer with weights tied to the input embeddings).\n",
        "!pip install --upgrade mxnet>=1.6.0\n",
        "!pip install gluonnlp\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer, SentencepieceDetokenizer\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 29.6 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 51 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 61 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 71 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 81 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 92 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 184 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 194 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 204 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 215 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 225 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 235 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 245 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 256 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 266 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 276 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 286 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 296 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 307 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 317 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 327 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 337 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 344 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.24)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595731 sha256=438ec50992c128491373312e3198ac293791c4a1ee4f7651a5512b492a1064e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 23.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 8.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5PMEDjk38lD"
      },
      "source": [
        "#%cd '/content/drive/MyDrive/mulcam_project/BERT_idiom'\n",
        "MY_PATH = '/content/drive/MyDrive/mulcam_project/BERT_idiom'\n",
        "MODEL_PATH = MY_PATH + 'gpt_ckpt'\n",
        "TOKENIZER_PATH = MY_PATH + 'gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH, num_best=0, alpha=0)\n",
        "detokenizer = SentencepieceDetokenizer(TOKENIZER_PATH)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token = None,\n",
        "                                               sep_token = None,\n",
        "                                               cls_token = None,\n",
        "                                               unknown_token = '<unk>',\n",
        "                                               padding_token = '<pad>',\n",
        "                                               bos_token = '<s>',\n",
        "                                               eos_token = '</s>')\n",
        "# vocab --> Vocab(size=50000, unk=\"<unk>\", reserved=\"['<pad>', '<s>', '</s>']\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGaeL8Mb4Hsc"
      },
      "source": [
        "\n",
        "# tokenizer 연습\n",
        "# 참고 : https://nlp.gluon.ai/api/modules/data.html\n",
        "toked = tokenizer('안녕 하세요')   # tokenizer가 잘못 만들어진 듯 ... 한글자씩 ?\n",
        "print(toked)\n",
        "\n",
        "toked_idx = vocab(toked)\n",
        "print(toked_idx)\n",
        "\n",
        "toked = vocab.to_tokens(toked_idx)\n",
        "print(toked)\n",
        "\n",
        "detoked = detokenizer(toked)\n",
        "print(detoked)\n",
        "\n",
        "''.join(toked).replace('▁', ' ')\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained(MODEL_PATH)\n",
        "model.summary()\n",
        "\n",
        "# 모델의 seed 입력 문장 생성\n",
        "tok = tokenizer('이때')   # tok = ['▁', '이', '때']\n",
        "tok_idx = [vocab[vocab.bos_token]] + vocab[tok]     # tok_idx = [0, 47437, 47438, 47675]\n",
        "input_ids = tf.convert_to_tensor(tok_idx)[None, :]  # 텐서로 변환\n",
        "\n",
        "# 모델의 출력\n",
        "output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cABPntLG4JTr"
      },
      "source": [
        "# 모델의 출력을 문자열로 변환\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpB1VNzS4Juj"
      },
      "source": [
        "# Beam search\n",
        "output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEdaPJPr4KkK"
      },
      "source": [
        "# 연속된 단어가 나오는 것을 방지함. no_repeat_ngram_size = 2\n",
        "output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97lKG8H34LjT"
      },
      "source": [
        "# top_k sampling\n",
        "output = model.generate(input_ids, max_length=50, do_sample = True, top_k=100, temperature=0.8)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I9wYhCt4NS7"
      },
      "source": [
        "# top_p sampling\n",
        "output = model.generate(input_ids, max_length=50, do_sample = True, top_p=0.9, temperature=0.8)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBS1nzBo4OTi"
      },
      "source": [
        "# top_k & top_p sampling\n",
        "output = model.generate(input_ids, max_length=50, do_sample = True, top_k=100, top_p=0.9, temperature=0.8)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdld4eib4Od6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}